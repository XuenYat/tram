import torch
import torch.nn as nn
import sys
import os
import einops
from typing import Dict, Tuple, Optional

# æ·»åŠ VGGTè·¯å¾„
sys.path.append('thirdparty/vggt')

# å¯¼å…¥VGGT
from vggt.models.vggt import VGGT
from lib.models.hmr_vimo import HMR_VIMO

class VGGTTransferLearning:
    """VGGTè¿ç§»å­¦ä¹ å·¥å…·ç±»ï¼ˆåŸºäºçœŸå®ç»“æ„ï¼‰"""
    
    def __init__(self, model_path: str = "~/.cache/torch/hub/checkpoints/model.pt"):
        self.model_path = os.path.expanduser(model_path)
        self.output_dir = "/workspace/tram/data/pretrain"
        os.makedirs(self.output_dir, exist_ok=True)
    
    def extract_and_save_weights(self, save_dir="./vggt_weights") -> Tuple[str, str, str]:
        """æå–VGGTçš„aggregatorå’Œcamera_headæƒé‡"""
        
        print("æ­£åœ¨åŠ è½½VGGTé¢„è®­ç»ƒæ¨¡å‹...")
        model = VGGT()
        
        if os.path.exists(self.model_path):
            state_dict = torch.load(self.model_path, map_location='cpu')
            model.load_state_dict(state_dict)
            print(f"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {self.model_path}")
        else:
            print(f"âš ï¸  é¢„è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
            state_dict = model.state_dict()
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # æƒé‡åˆ†ç±» - åŸºäºå®é™…çš„VGGTç»“æ„
        aggregator_weights = {}  # backboneç‰¹å¾æå–å™¨
        camera_weights = {}      # ç›¸æœºå‚æ•°é¢„æµ‹å™¨
        other_weights = {}       # å…¶ä»–ç»„ä»¶
        
        print("\nåˆ†æVGGTæ¨¡å‹æƒé‡...")
        
        for key, value in state_dict.items():
            if key.startswith('aggregator.'):
                # aggregatoræ˜¯ä¸»è¦çš„ç‰¹å¾æå–å™¨ï¼ˆbackboneï¼‰
                aggregator_weights[key] = value
                #print(f"Aggregator: {key} {list(value.shape)}")
                
            elif key.startswith('camera_head.'):
                # camera_headæ˜¯ç›¸æœºå‚æ•°é¢„æµ‹å™¨
                camera_weights[key] = value
                #print(f"Camera: {key} {list(value.shape)}")
                
            else:
                # å…¶ä»–ç»„ä»¶ï¼ˆdepth_head, point_head, track_headç­‰ï¼‰
                other_weights[key] = value
                #print(f"Other: {key} {list(value.shape)}")
        
        # ä¿å­˜æƒé‡æ–‡ä»¶
        aggregator_path = os.path.join(save_dir, "vggt_aggregator.pth")
        camera_path = os.path.join(save_dir, "vggt_camera_head.pth")
        full_path = os.path.join(save_dir, "vggt_full_model.pth")
        
        torch.save(aggregator_weights, aggregator_path)
        torch.save(camera_weights, camera_path)
        torch.save(state_dict, full_path)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._print_weight_statistics(aggregator_weights, camera_weights, other_weights)
        
        return aggregator_path, camera_path, full_path
    
    def _print_weight_statistics(self, aggregator_weights: Dict, camera_weights: Dict, other_weights: Dict):
        """æ‰“å°æƒé‡ç»Ÿè®¡ä¿¡æ¯"""
        aggregator_params = sum(p.numel() for p in aggregator_weights.values())
        camera_params = sum(p.numel() for p in camera_weights.values())
        other_params = sum(p.numel() for p in other_weights.values())
        total_params = aggregator_params + camera_params + other_params
        
        print(f"\n=== æƒé‡æå–å®Œæˆ ===")
        print(f"Aggregatoræƒé‡: {aggregator_params:,} ({aggregator_params/1e6:.1f}M) å‚æ•°")
        print(f"ç›¸æœºå¤´æƒé‡: {camera_params:,} ({camera_params/1e6:.1f}M) å‚æ•°")
        print(f"å…¶ä»–æƒé‡: {other_params:,} ({other_params/1e6:.1f}M) å‚æ•°")
        print(f"æ€»å‚æ•°: {total_params:,} ({total_params/1e6:.1f}M)")
        print(f"æƒé‡æ¯”ä¾‹: Aggregator {aggregator_params/total_params*100:.1f}%, Camera {camera_params/total_params*100:.1f}%")

class TokenAdapter(nn.Module):
    """Tokenæ•°é‡å’Œç‰¹å¾ç»´åº¦é€‚é…å±‚ï¼š[B, 192, 1280] â†’ [B, 196, 2048]"""
    def __init__(self, input_dim=1280, output_dim=2048):
        super().__init__()
        # 4ä¸ªå¯å­¦ä¹ çš„è¡¥å……token
        self.additional_tokens = nn.Parameter(torch.randn(1, 4, input_dim) * 0.02)
        
        # ç‰¹å¾ç»´åº¦æ˜ å°„ï¼š1280 â†’ 2048
        self.feature_projection = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU()
        )
        
    def forward(self, x): 
        # x: [B, 192, 1280]
        B, T, D = x.shape
        assert T == 192, f"æœŸæœ›192ä¸ªtokensï¼Œä½†å¾—åˆ°{T}ä¸ª"
        assert D == 1280, f"æœŸæœ›1280ç»´ç‰¹å¾ï¼Œä½†å¾—åˆ°{D}ç»´"
        
        # 1. æ·»åŠ 4ä¸ªtokensï¼š192 â†’ 196
        additional_tokens = self.additional_tokens.expand(B, -1, -1)  # [B, 4, 1280]
        x_expanded = torch.cat([x, additional_tokens], dim=1)  # [B, 196, 1280]
        
        # 2. ç‰¹å¾ç»´åº¦æ˜ å°„ï¼š1280 â†’ 2048
        output = self.feature_projection(x_expanded)  # [B, 196, 2048]
        
        return output

class TRAMWithVGGTTransfer(nn.Module):
    """é›†æˆVGGTè¿ç§»å­¦ä¹ çš„TRAMæ¨¡å‹ï¼ˆåŸºäºçœŸå®ç»“æ„ï¼‰"""
    
    def __init__(self,
                backbone_path: str,
                camera_path: str,
                cfg,  # éœ€è¦æ·»åŠ cfgå‚æ•°
                freeze_backbone: bool = True,
                camera_fine_tune: bool = True):
        super().__init__()
        
        from lib.models.smpl import SMPL
        self.smpl = SMPL()
        self.crop_size = 256
        self.seq_len = 16
        
        self.freeze_backbone = freeze_backbone
        self.camera_fine_tune = camera_fine_tune
        
        # === æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨TRAM backboneæ›¿ä»£VGGT aggregator ===
        # 1. åŠ è½½åŸå§‹TRAMæ¨¡å‹æ¥è·å–å¼ºå¤§çš„backbone
        
        try:
            # 1. åˆ›å»ºTRAMæ¨¡å‹
            tram_model = HMR_VIMO(cfg=cfg)
            print("âœ… TRAMæ¨¡å‹åˆ›å»ºæˆåŠŸ")

            # === æ·»åŠ æ£€æŸ¥2: é…ç½®æ£€æŸ¥ ===
            print("=== æ£€æŸ¥é…ç½® ===")
            print(f"cfg.MODEL.ST_MODULE: {cfg.MODEL.ST_MODULE}")
            print(f"cfg.MODEL.MOTION_MODULE: {cfg.MODEL.MOTION_MODULE}")
            print(f"cfg.MODEL.ST_HDIM: {cfg.MODEL.ST_HDIM}")
            print(f"cfg.MODEL.MOTION_HDIM: {cfg.MODEL.MOTION_HDIM}")
            print(f"cfg.MODEL.ST_NLAYER: {cfg.MODEL.ST_NLAYER}")
            print(f"cfg.MODEL.MOTION_NLAYER: {cfg.MODEL.MOTION_NLAYER}")

            # 2. å¼ºåˆ¶åŠ è½½é¢„è®­ç»ƒæƒé‡
            if hasattr(cfg.MODEL, 'CHECKPOINT') and cfg.MODEL.CHECKPOINT:
                checkpoint_path = cfg.MODEL.CHECKPOINT
                print(f"ğŸ”„ å¼ºåˆ¶åŠ è½½TRAMæƒé‡: {checkpoint_path}")

                # === æ·»åŠ æ£€æŸ¥3: checkpointå†…å®¹æ£€æŸ¥ ===
                print("=== æ£€æŸ¥checkpointå†…å®¹ ===")
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                    
                    st_keys = [k for k in state_dict.keys() if 'st_module' in k]
                    motion_keys = [k for k in state_dict.keys() if 'motion_module' in k]
                    
                    print(f"ST module keys: {len(st_keys)}")
                    print(f"Motion module keys: {len(motion_keys)}")
                    
                    if len(st_keys) > 0:
                        print("å‰å‡ ä¸ªST keys:", st_keys[:5])
                    if len(motion_keys) > 0:
                        print("å‰å‡ ä¸ªMotion keys:", motion_keys[:5])

                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                    
                    # åŠ è½½æƒé‡åˆ°TRAMæ¨¡å‹
                    missing_keys, unexpected_keys = tram_model.load_state_dict(
                        state_dict, strict=False
                    )
                    print(f"âœ… TRAMæƒé‡åŠ è½½å®Œæˆ:")
                    print(f"   - Missing keys: {len(missing_keys)}")
                    print(f"   - Unexpected keys: {len(unexpected_keys)}")

                    # === æ·»åŠ æ£€æŸ¥1: æ¨¡å—ç»§æ‰¿æ£€æŸ¥ ===
                    print("=== æ£€æŸ¥åŸå§‹TRAMæ¨¡å‹ç»“æ„ ===")
                    print(f"åŸå§‹TRAMæ€»å‚æ•°: {sum(p.numel() for p in tram_model.parameters()):,}")

                    # è¯¦ç»†æ£€æŸ¥æ¯ä¸ªç»„ä»¶
                    components_to_check = ['backbone', 'st_module', 'motion_module', 'smpl_head']
                    for comp_name in components_to_check:
                        if hasattr(tram_model, comp_name):
                            comp = getattr(tram_model, comp_name)
                            if comp is not None:
                                params = sum(p.numel() for p in comp.parameters())
                                print(f"{comp_name}: {params:,} å‚æ•°")
                            else:
                                print(f"{comp_name}: None")
                        else:
                            print(f"{comp_name}: ä¸å­˜åœ¨")

                    if len(missing_keys) > 0:
                        print(f"   - éƒ¨åˆ†missing keys: {missing_keys[:5]}...")
            
            # 3. ç°åœ¨æµ‹è¯•backboneï¼ˆåº”è¯¥å¯ä»¥å·¥ä½œäº†ï¼‰
            test_input = torch.randn(1, 3, 256, 192)
            with torch.no_grad():
                test_output = tram_model.backbone(test_input)
                print(f"âœ… åŠ è½½æƒé‡åbackboneæµ‹è¯•æˆåŠŸ: {test_output.shape}")
            
            self.tram_backbone = tram_model.backbone
            self.tram_model = tram_model
            
        except Exception as e:
            print(f"âŒ TRAM backboneä»ç„¶å¤±è´¥: {e}")
            print("ä½¿ç”¨å¤‡ç”¨ResNet backbone")
        

        print("=== è°ƒè¯•æƒé‡åŠ è½½ ===")
        print("Missing keysè¯¦ç»†ä¿¡æ¯:")
        for i, key in enumerate(missing_keys):
            if i < 20:  # æ‰“å°å‰20ä¸ª
                print(f"  {key}")
            elif i == 20:
                print("  ...")
                break

        # æ£€æŸ¥backboneæƒé‡æ˜¯å¦çœŸçš„åŠ è½½äº†
        print("æ£€æŸ¥backboneç¬¬ä¸€å±‚æƒé‡:")
        first_weight = None
        for name, param in tram_model.backbone.named_parameters():
            if 'weight' in name:
                first_weight = param
                print(f"  {name}: mean={param.mean().item():.6f}, std={param.std().item():.6f}")
                break

        # æ£€æŸ¥checkpointä¸­æ˜¯å¦æœ‰backboneæƒé‡
        backbone_keys_in_checkpoint = [k for k in state_dict.keys() if k.startswith('backbone.')]
        print(f"Checkpointä¸­backboneç›¸å…³keysæ•°é‡: {len(backbone_keys_in_checkpoint)}")
        if len(backbone_keys_in_checkpoint) > 0:
            print("å‰5ä¸ªbackbone keys:")
            for key in backbone_keys_in_checkpoint[:5]:
                print(f"  {key}")

        # 2. åªåŠ è½½VGGTçš„ç›¸æœºå¤´ï¼ˆä¿æŒä¸“ä¸šç›¸æœºé¢„æµ‹èƒ½åŠ›ï¼‰
        self.vggt_camera_head = self._load_camera_head(camera_path)

        #ç‰¹å¾é€‚é…ï¼šTRAMè¾“å‡º(192 tokens) â†’ VGGTæœŸæœ›æ ¼å¼(196 tokens)
        self.feature_adapter = self._init_feature_adapter()

        if hasattr(tram_model, 'smpl_head'):
            self.tram_smpl_head = tram_model.smpl_head
            print("âœ… ä½¿ç”¨åŸå§‹TRAM SMPL head")
        else:
            raise ValueError("TRAMæ¨¡å‹æ²¡æœ‰smpl_headç»„ä»¶")
        
        # åœ¨æ¨¡å‹åˆå§‹åŒ–ä¸­æ·»åŠ æ£€æŸ¥ï¼š
        if hasattr(tram_model, 'st_module') and tram_model.st_module is not None:
            self.st_module = tram_model.st_module
            print("âœ… ST Moduleå·²ç»§æ‰¿")
        else:
            self.st_module = None
            print("âš ï¸ ST Moduleæœªæ‰¾åˆ°")

        if hasattr(tram_model, 'motion_module') and tram_model.motion_module is not None:
            self.motion_module = tram_model.motion_module  
            print("âœ… Motion Moduleå·²ç»§æ‰¿")
        else:
            self.motion_module = None
            print("âš ï¸ Motion Moduleæœªæ‰¾åˆ°")

        # 4. è®¾ç½®æ··åˆæ¶æ„çš„è®­ç»ƒç­–ç•¥
        self._setup_hybrid_training_strategy()
        
        print("âœ… æ··åˆTRAM-VGGTæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def _setup_hybrid_training_strategy(self):
        """è®¾ç½®æ··åˆæ¶æ„çš„è®­ç»ƒç­–ç•¥"""
        # è®¾ç½®TRAM backbone
        if self.freeze_backbone:
            for param in self.tram_backbone.parameters():
                param.requires_grad = False
            print("âœ“ TRAM Backboneå·²å†»ç»“")
        else:
            for param in self.tram_backbone.parameters():
                param.requires_grad = True
            print("âœ“ TRAM Backboneå…è®¸è®­ç»ƒ")
        
        # è®¾ç½®VGGTç›¸æœºå¤´å¾®è°ƒ
        if self.camera_fine_tune:
            for param in self.vggt_camera_head.parameters():
                param.requires_grad = True
            print("âœ“ VGGT Camera Headå…è®¸å¾®è°ƒ")
        else:
            for param in self.vggt_camera_head.parameters():
                param.requires_grad = False
            print("âœ“ VGGT Camera Headå·²å†»ç»“")
        
        # TRAM SMPLå¤´é»˜è®¤å¯è®­ç»ƒ
        for param in self.tram_smpl_head.parameters():
            param.requires_grad = True
        print("âœ“ TRAM SMPL Headå…è®¸è®­ç»ƒ")

        # æ·»åŠ Motionå’ŒSTæ¨¡å—çš„è®­ç»ƒè®¾ç½®
        if hasattr(self, 'motion_module') and self.motion_module is not None:
            for param in self.motion_module.parameters():
                param.requires_grad = True
            print("âœ“ Motion Moduleå…è®¸è®­ç»ƒ")
        
        if hasattr(self, 'st_module') and self.st_module is not None:
            for param in self.st_module.parameters():
                param.requires_grad = True  
            print("âœ“ ST Moduleå…è®¸è®­ç»ƒ")

        for param in self.feature_adapter.parameters():
            param.requires_grad = True
        print("âœ“ ç‰¹å¾é€‚é…å±‚å…è®¸è®­ç»ƒ")

    def project(self, points, pred_cam, center, scale, img_focal, img_center, return_full=False):
        # ç›´æ¥å¤åˆ¶HMR_VIMOçš„projectæ–¹æ³•
        return HMR_VIMO.project(self, points, pred_cam, center, scale, img_focal, img_center, return_full)
    
    def get_trans(self, pred_cam, center, scale, img_focal, img_center):
        return HMR_VIMO.get_trans(self, pred_cam, center, scale, img_focal, img_center)
  
    def _load_camera_head(self, camera_path: str) -> nn.Module:
        """åŠ è½½VGGT camera_head"""
        print(f"åŠ è½½VGGT camera_head from {camera_path}")
        
        # åˆ›å»ºå®Œæ•´çš„VGGTæ¨¡å‹æ¥è·å–camera_head
        vggt_model = VGGT()
        camera_head = vggt_model.camera_head
        
        try:
            # åŠ è½½camera_headæƒé‡
            camera_weights = torch.load(camera_path, map_location='cpu')
            
            # åªåŠ è½½camera_headç›¸å…³çš„æƒé‡
            camera_state_dict = {}
            for key, value in camera_weights.items():
                if key.startswith('camera_head.'):
                    # ç§»é™¤'camera_head.'å‰ç¼€
                    new_key = key[12:]  # len('camera_head.') = 12
                    camera_state_dict[new_key] = value
            
            missing_keys, unexpected_keys = camera_head.load_state_dict(camera_state_dict, strict=False)
            
            if missing_keys:
                print(f"Warning: Missing keys in camera_head: {len(missing_keys)} keys")
            if unexpected_keys:
                print(f"Warning: Unexpected keys in camera_head: {len(unexpected_keys)} keys")
            
            print(f"âœ… Camera headæƒé‡åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸  Camera headæƒé‡åŠ è½½å¤±è´¥: {e}")
            print("ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
        
        return camera_head

    def _init_feature_adapter(self) -> nn.Module:
        """åˆå§‹åŒ–ç‰¹å¾é€‚é…å±‚ï¼š[B, 192, 1280] â†’ [B, 196, 2048]"""
        return nn.Sequential(
            TokenAdapter(input_dim=1280, output_dim=2048),  # æ ¸å¿ƒé€‚é…
            nn.Dropout(0.1)                                 # æ­£åˆ™åŒ–
        )

    def _setup_training_strategy(self):
        """è®¾ç½®è®­ç»ƒç­–ç•¥"""
        # å†»ç»“aggregator
        if self.freeze_backbone:
            for param in self.vggt_aggregator.parameters():
                param.requires_grad = False
            print("âœ“ VGGT Aggregatorå·²å†»ç»“")
        else:
            for param in self.vggt_aggregator.parameters():
                param.requires_grad = True
            print("âœ“ VGGT Aggregatorå…è®¸è®­ç»ƒ")
        
        # è®¾ç½®camera headå¾®è°ƒ
        if self.camera_fine_tune:
            for param in self.vggt_camera_head.parameters():
                param.requires_grad = True
            print("âœ“ VGGT Camera Headå…è®¸å¾®è°ƒ")
        else:
            for param in self.vggt_camera_head.parameters():
                param.requires_grad = False
            print("âœ“ VGGT Camera Headå·²å†»ç»“")
        
        # TRAMç»„ä»¶é»˜è®¤å¯è®­ç»ƒ
        for param in self.tram_smpl_head.parameters():
            param.requires_grad = True
        print("âœ“ TRAM SMPL Headå…è®¸è®­ç»ƒ")
    
    def forward(self, batch=None, images=None, iters=1, **kwargs):
        # å¤„ç†ä¸åŒçš„è¾“å…¥æ ¼å¼
        if batch is not None and 'img' in batch:
            images = batch['img']
            center = batch['center']
            scale = batch['scale']
            img_focal = batch['img_focal']
            img_center = batch['img_center']
        elif images is not None:
            # å¦‚æœåªæä¾›imagesï¼Œåˆ›å»ºé»˜è®¤çš„ç›¸æœºå‚æ•°
            batch_size = images.size(0)
            device = images.device
            center = torch.tensor([[self.crop_size/2, self.crop_size/2]]).repeat(batch_size, 1).to(device)
            scale = torch.ones(batch_size).to(device) 
            img_focal = torch.ones(batch_size).to(device) * 1000.0
            img_center = torch.tensor([[self.crop_size/2, self.crop_size/2]]).repeat(batch_size, 1).to(device)
        else:
            raise ValueError("éœ€è¦æä¾› batch æˆ– images å‚æ•°")

        # === æ·»åŠ è°ƒè¯•ä¿¡æ¯ ===
        # print(f"DEBUG: è¾“å…¥å›¾åƒshape: {images.shape}")

        # å¦‚æœè¾“å…¥æ˜¯256Ã—256ï¼Œç«‹å³è£å‰ªä¸º256Ã—192
        if images.shape[-2:] == (256, 256):
            # print("DEBUG: æ£€æµ‹åˆ°256Ã—256è¾“å…¥ï¼Œç«‹å³è£å‰ªä¸º256Ã—192")
            images = images[:, :, :, 32:-32]  # è£å‰ªä¸º256Ã—192
            # print(f"DEBUG: è£å‰ªåå°ºå¯¸: {images.shape}")

        batch_size = images.size(0)

        # === ç°åœ¨ä½¿ç”¨æ­£ç¡®å°ºå¯¸çš„å›¾åƒè°ƒç”¨backbone ===
        tram_features = self.tram_backbone(images)  # ç°åœ¨åº”è¯¥æ˜¯256Ã—192çš„è¾“å…¥

        if self.st_module is not None:
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ—¶åºæ¡ä»¶ï¼ˆ16çš„å€æ•°ï¼‰
            if batch_size % 16 == 0:
                #print(f"ğŸ”„ è°ƒç”¨ST Moduleå¤„ç† {batch_size} å¸§")
                
                # ä½¿ç”¨ä¿å­˜çš„tram_modelè°ƒç”¨bbox_est
                bbox_info = self.tram_model.bbox_est(center, scale, img_focal, img_center)  # [B, 3]
                
                # æŒ‰ç…§åŸå§‹TRAMçš„æ–¹å¼å¤„ç†STæ¨¡å—
                bb = einops.repeat(bbox_info, 'b c -> b c h w', h=16, w=12)  # [B, 3, 16, 12]
                st_input = torch.cat([tram_features, bb], dim=1)  # [B, 1283, 16, 12]

                # é‡å¡‘ä¸ºæ—¶åºæ ¼å¼å¹¶è°ƒç”¨STæ¨¡å—
                st_input = einops.rearrange(st_input, '(b t) c h w -> (b h w) t c', t=16)
                st_output = self.st_module(st_input)  # [(B*H*W), 16, 1280]
                tram_features = einops.rearrange(st_output, '(b h w) t c -> (b t) c h w', h=16, w=12)
                
                #print("âœ… ST Moduleå¤„ç†å®Œæˆ")
            else:
                print(f"âš ï¸ æ‰¹æ¬¡å¤§å° {batch_size} ä¸æ˜¯16çš„å€æ•°ï¼Œè·³è¿‡ST Module")

        # print(f"DEBUG: TRAMç‰¹å¾shape: {tram_features.shape}")
        
        # 1.5 ç‰¹å¾é€‚é…ï¼šTRAMè¾“å‡º(192 tokens) â†’ VGGTæœŸæœ›æ ¼å¼(196 tokens)
        assert tram_features.dim() == 4, f"æœŸæœ›TRAM backboneè¾“å‡º4ç»´ç‰¹å¾ï¼Œå®é™…å¾—åˆ°{tram_features.dim()}ç»´"
        B, C, H, W = tram_features.shape
        assert H == 16 and W == 12, f"æœŸæœ›16Ã—12ç©ºé—´å¸ƒå±€ï¼Œå®é™…å¾—åˆ°{H}Ã—{W}"
        assert C == 1280, f"æœŸæœ›ViT-hugeç‰¹å¾ç»´åº¦1280ï¼Œå®é™…å¾—åˆ°{C}"

        # è½¬æ¢ä¸ºåºåˆ—æ ¼å¼ [B, 192, 1280]
        tram_features_seq = tram_features.reshape(B, C, -1).transpose(1, 2)

        # ç‰¹å¾é€‚é…ï¼š192 tokens â†’ 196 tokens, 1280 dim â†’ 2048 dim
        adapted_features = self.feature_adapter(tram_features_seq)  # [B, 196, 2048]

        # print(f"DEBUG: é€‚é…åç‰¹å¾shape: {adapted_features.shape}")

        # === å…³é”®ä¿®æ”¹ï¼šåˆ›å»ºä¸“ç”¨ç›¸æœºtoken ===
        # åˆ›å»ºå…¨å±€æ„ŸçŸ¥çš„ç›¸æœºtokenï¼ˆæ‰€æœ‰å›¾åƒpatchçš„å…¨å±€è¡¨ç¤ºï¼‰
        global_camera_token = adapted_features.mean(dim=1, keepdim=True)  # [B, 1, 2048]

        # é‡æ–°ç»„ç»‡ä¸ºVGGTæœŸæœ›æ ¼å¼ï¼š[ç›¸æœºtoken, å›¾åƒtokens]
        vggt_tokens = torch.cat([global_camera_token, adapted_features], dim=1)  # [B, 197, 2048]

        # æ·»åŠ åºåˆ—ç»´åº¦ï¼ˆVGGTæœŸæœ› [B, S, N, 2048] æ ¼å¼ï¼‰
        vggt_tokens = vggt_tokens.unsqueeze(1)  # [B, 1, 197, 2048]

        # print(f"DEBUG: VGGT tokens shape: {vggt_tokens.shape}")
        # print(f"DEBUG: ç›¸æœºtokenæµ‹è¯•: {vggt_tokens[:, :, 0].shape}")  # åº”è¯¥æ˜¯[B, 1, 2048]

        # 3. VGGTç›¸æœºå¤´å¤„ç†
        # åˆ›å»º24å±‚aggregatorè¾“å‡ºï¼ˆVGGTæœŸæœ›transformerçš„24å±‚è¾“å‡ºï¼‰
        aggregated_tokens_list = [vggt_tokens] * 24
        camera_predictions = self.vggt_camera_head(aggregated_tokens_list)

        # === æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯ ===
        # print(f"DEBUG: aggregated_tokens_listé•¿åº¦: {len(aggregated_tokens_list)}")
        # print(f"DEBUG: aggregated_tokens_list[0]çš„shape: {aggregated_tokens_list[0].shape}")
        # print(f"DEBUG: aggregated_tokens_list[0]çš„æ•°æ®ç±»å‹: {type(aggregated_tokens_list[0])}")

        camera_params = camera_predictions[-1]  # [B, 1, 9] æˆ– [B, 9]

        # print(f"DEBUG: åŸå§‹ç›¸æœºé¢„æµ‹shape: {camera_params.shape}")

        # ç¡®ä¿ç»´åº¦æ­£ç¡®
        if camera_params.dim() == 3 and camera_params.size(1) == 1:
            camera_params = camera_params.squeeze(1)  # [B, 9]
        elif camera_params.dim() != 2:
            raise ValueError(f"æ„å¤–çš„ç›¸æœºå‚æ•°ç»´åº¦: {camera_params.shape}")

        # print(f"DEBUG: æœ€ç»ˆç›¸æœºå‚æ•°shape: {camera_params.shape}")
        
        # 4. TRAM SMPLå¤„ç† - ä½¿ç”¨åŸå§‹TRAMæ–¹å¼
        # print(f"DEBUG: TRAMç‰¹å¾åŸå§‹shape: {tram_features.shape}")  # [B, 1280, 16, 12]

        # ç›´æ¥ä½¿ç”¨4ç»´ç‰¹å¾ï¼Œå°±åƒåŸå§‹TRAMä¸€æ ·
        pred_pose, pred_shape, pred_cam_tram_original = self.tram_smpl_head(tram_features)

        # === ç›¸æœºå‚æ•°å¯¹æ¯”è°ƒè¯• ===
        pred_cam_tram = self.convert_vggt_to_tram_camera(camera_params)

        # print(f"ğŸ” ç›¸æœºå‚æ•°å¯¹æ¯”:")
        # print(f"  åŸå§‹TRAMç›¸æœº: mean={pred_cam_tram_original.mean(dim=0)}, std={pred_cam_tram_original.std(dim=0)}")
        # print(f"  VGGTè½¬æ¢ç›¸æœº: mean={pred_cam_tram.mean(dim=0)}, std={pred_cam_tram.std(dim=0)}")

        # === å…³é”®ä¿®å¤ï¼šæ·»åŠ Motionæ¨¡å—è°ƒç”¨ ===
        if self.motion_module is not None:
            if batch_size % 16 == 0:
                #print(f"ğŸ”„ è°ƒç”¨Motion Moduleå¤„ç† {batch_size} å¸§çš„pose")
                
                # ä½¿ç”¨ä¿å­˜çš„tram_modelè°ƒç”¨bbox_est
                bbox_info = self.tram_model.bbox_est(center, scale, img_focal, img_center)  # [B, 3]
                bb = einops.rearrange(bbox_info, '(b t) c -> b t c', t=16)  # [B/16, 16, 3]
                pred_pose_seq = einops.rearrange(pred_pose, '(b t) c -> b t c', t=16)  # [B/16, 16, 144]
                
                motion_input = torch.cat([pred_pose_seq, bb], dim=2)  # [B/16, 16, 147]
                motion_output = self.motion_module(motion_input)  # [B/16, 16, 144]
                pred_pose = einops.rearrange(motion_output, 'b t c -> (b t) c')  # [B, 144]
                
                #print("âœ… Motion Moduleå¤„ç†å®Œæˆ")
            else:
                print(f"âš ï¸ æ‰¹æ¬¡å¤§å° {batch_size} ä¸æ˜¯16çš„å€æ•°ï¼Œè·³è¿‡Motion Module")

        # print(f"DEBUG: åŸå§‹TRAM SMPLè¾“å‡º:")
        # print(f"  - pred_pose shape: {pred_pose.shape}")
        # print(f"  - pred_shape shape: {pred_shape.shape}")
        # print(f"  - pred_cam_tram shape: {pred_cam_tram_original.shape}")

        # æ³¨æ„ï¼šæˆ‘ä»¬å¿½ç•¥pred_cam_tram_originalï¼Œä½¿ç”¨VGGTçš„ç›¸æœºå‚æ•°

        # 6. ä»SMPLå‚æ•°ç”Ÿæˆ3Då…³é”®ç‚¹
        from lib.utils.geometry import rot6d_to_rotmat_hmr2 as rot6d_to_rotmat
        pred_rotmat = rot6d_to_rotmat(pred_pose).reshape(batch_size, 24, 3, 3)

        smpl_output = self.smpl(
            global_orient=pred_rotmat[:, [0]],
            body_pose=pred_rotmat[:, 1:],
            betas=pred_shape,
            pose2rot=False
        )
        pred_keypoints_3d = smpl_output.joints  # [B, 49, 3]
        
        # 7. å…³é”®æ­¥éª¤ï¼šå°†VGGTç›¸æœºå‚æ•°è½¬æ¢ä¸ºTRAMæ ¼å¼ç”¨äºæŠ•å½±
        pred_cam_tram = self.convert_vggt_to_tram_camera(camera_params)  # [B, 3]
        
        # 8. ä½¿ç”¨VGGTæŒ‡å¯¼çš„ç›¸æœºå‚æ•°è¿›è¡Œ2DæŠ•å½±
        center = torch.tensor([[self.crop_size/2, self.crop_size/2]]).repeat(batch_size, 1).to(images.device)
        scale = torch.ones(batch_size).to(images.device) 
        img_focal = torch.ones(batch_size).to(images.device) * 1000.0
        img_center = torch.tensor([[self.crop_size/2, self.crop_size/2]]).repeat(batch_size, 1).to(images.device)
        
        pred_keypoints_2d = self.project(
            pred_keypoints_3d,    # 3Då…³é”®ç‚¹
            pred_cam_tram,        # ä½¿ç”¨VGGTæŒ‡å¯¼çš„ç›¸æœºå‚æ•°ï¼
            center, scale, img_focal, img_center
        )

        # 9. è¾“å‡ºç»“æœ
        output = {
            'pred_pose': pred_pose,
            'pred_shape': pred_shape,
            'pred_cam': pred_cam_tram,        # è¾“å‡ºVGGTæŒ‡å¯¼çš„ç›¸æœºå‚æ•°ï¼ˆTRAMæ ¼å¼ï¼‰
            'pred_camera': camera_params,     # åŸå§‹VGGTç›¸æœºå‚æ•°ï¼ˆ9ç»´æ ¼å¼ï¼‰
            'pred_keypoints_2d': pred_keypoints_2d,  # åŸºäºVGGTç›¸æœºçš„2DæŠ•å½±ï¼
            'pred_keypoints_3d': pred_keypoints_3d,
            'pred_rotmat': pred_rotmat,
            'pred_rotmat_0': pred_rotmat,
            'features': tram_features,    
        }

        # 10. å…¼å®¹åŸå§‹TRAMçš„è¿­ä»£è®­ç»ƒæ¥å£
        iter_preds = [
            [pred_rotmat] * iters,         # rotmat_preds
            [pred_shape] * iters,          # shape_preds  
            [pred_cam_tram] * iters,       # cam_preds - ä½¿ç”¨VGGTæŒ‡å¯¼çš„ç›¸æœºå‚æ•°
            [pred_keypoints_3d] * iters,   # j3d_preds
            [pred_keypoints_2d] * iters    # j2d_preds - åŸºäºVGGTç›¸æœºçš„æŠ•å½±
        ]
        
        return output, iter_preds

    def analyze_camera_parameters(self, vggt_camera, tram_camera_original, batch):
        """åˆ†æä¸¤ç§ç›¸æœºå‚æ•°çš„çœŸå®ç‰©ç†å«ä¹‰"""
        
        # è·å–å›¾åƒä¿¡æ¯
        center = batch['center']
        scale = batch['scale'] 
        img_focal = batch['img_focal']
        img_center = batch['img_center']
        
        print(f"\nğŸ” === ç›¸æœºå‚æ•°ç‰©ç†å«ä¹‰åˆ†æ ===")
        print(f"æ‰¹æ¬¡å¤§å°: {len(center)}")
        print(f"å›¾åƒä¿¡æ¯ (ç¬¬ä¸€ä¸ªæ ·æœ¬):")
        print(f"  center (cropä¸­å¿ƒ): [{center[0][0]:.1f}, {center[0][1]:.1f}]")
        print(f"  scale (cropç¼©æ”¾): {scale[0]:.3f}")   
        print(f"  img_focal (ç„¦è·): {img_focal[0]:.1f}")  
        print(f"  img_center (å›¾åƒä¸­å¿ƒ): [{img_center[0][0]:.1f}, {img_center[0][1]:.1f}]")
        print(f"  crop_size: {self.crop_size}")
        
        # åˆ†æVGGTå‚æ•° (å–å‰3ä¸ªæ ·æœ¬)
        print(f"\nVGGTç›¸æœºå‚æ•°åˆ†æ:")
        for i in range(min(3, len(vggt_camera))):
            vggt_trans = vggt_camera[i, :3]
            vggt_quat = vggt_camera[i, 3:7]
            vggt_fov = vggt_camera[i, 7:9]
            
            print(f"  æ ·æœ¬{i}: translation=[{vggt_trans[0]:.6f}, {vggt_trans[1]:.6f}, {vggt_trans[2]:.3f}]")
            print(f"         quaternion=[{vggt_quat[0]:.3f}, {vggt_quat[1]:.3f}, {vggt_quat[2]:.3f}, {vggt_quat[3]:.3f}]")
            print(f"         fov=[{vggt_fov[0]:.1f}Â°, {vggt_fov[1]:.1f}Â°]")
        
        # åˆ†æTRAMå‚æ•°
        print(f"\nTRAMç›¸æœºå‚æ•°åˆ†æ:")
        for i in range(min(3, len(tram_camera_original))):
            tram_params = tram_camera_original[i]
            print(f"  æ ·æœ¬{i}: [s={tram_params[0]:.3f}, tx={tram_params[1]:.3f}, ty={tram_params[2]:.3f}]")
        
        # === å…³é”®åˆ†æï¼šè®¡ç®—TRAMçš„å®é™…3D translation ===
        print(f"\nğŸ¯ 3Dåæ ‡å¯¹æ¯”åˆ†æ:")
        for i in range(min(3, len(vggt_camera))):
            # TRAMè®¡ç®—çš„3D translation
            tram_trans_3d = self.tram_model.get_trans(
                tram_camera_original[[i]], center[[i]], scale[[i]], 
                img_focal[[i]], img_center[[i]]
            )
            
            vggt_trans = vggt_camera[i, :3]
            tram_3d = tram_trans_3d[0, 0]  # [tx, ty, tz]
            
            print(f"  æ ·æœ¬{i}:")
            print(f"    VGGT 3D translation: [{vggt_trans[0]:.6f}, {vggt_trans[1]:.6f}, {vggt_trans[2]:.3f}]")
            print(f"    TRAM 3D translation: [{tram_3d[0]:.6f}, {tram_3d[1]:.6f}, {tram_3d[2]:.3f}]")
            print(f"    æ·±åº¦æ¯”å€¼ (VGGT/TRAM): {vggt_trans[2]/tram_3d[2]:.3f}")
            print(f"    XYå·®å¼‚: dx={abs(vggt_trans[0]-tram_3d[0]):.6f}, dy={abs(vggt_trans[1]-tram_3d[1]):.6f}")
        
        # === ç»Ÿè®¡åˆ†æ ===
        print(f"\nğŸ“Š ç»Ÿè®¡å¯¹æ¯”:")
        
        # VGGTç»Ÿè®¡
        vggt_trans_all = vggt_camera[:, :3]
        print(f"VGGT translationç»Ÿè®¡:")
        print(f"  X: mean={vggt_trans_all[:, 0].mean():.6f}, std={vggt_trans_all[:, 0].std():.6f}")
        print(f"  Y: mean={vggt_trans_all[:, 1].mean():.6f}, std={vggt_trans_all[:, 1].std():.6f}")
        print(f"  Z: mean={vggt_trans_all[:, 2].mean():.3f}, std={vggt_trans_all[:, 2].std():.3f}")
        
        # TRAMç»Ÿè®¡
        print(f"TRAMå‚æ•°ç»Ÿè®¡:")
        print(f"  s: mean={tram_camera_original[:, 0].mean():.3f}, std={tram_camera_original[:, 0].std():.3f}")
        print(f"  tx: mean={tram_camera_original[:, 1].mean():.3f}, std={tram_camera_original[:, 1].std():.3f}")
        print(f"  ty: mean={tram_camera_original[:, 2].mean():.3f}, std={tram_camera_original[:, 2].std():.3f}")
        
        # TRAM 3D translationç»Ÿè®¡
        all_tram_3d = []
        for i in range(len(tram_camera_original)):
            tram_3d = self.tram_model.get_trans(
                tram_camera_original[[i]], center[[i]], scale[[i]], 
                img_focal[[i]], img_center[[i]]
            )
            all_tram_3d.append(tram_3d[0, 0])
        
        all_tram_3d = torch.stack(all_tram_3d)
        print(f"TRAM 3D translationç»Ÿè®¡:")
        print(f"  X: mean={all_tram_3d[:, 0].mean():.6f}, std={all_tram_3d[:, 0].std():.6f}")
        print(f"  Y: mean={all_tram_3d[:, 1].mean():.6f}, std={all_tram_3d[:, 1].std():.6f}")
        print(f"  Z: mean={all_tram_3d[:, 2].mean():.3f}, std={all_tram_3d[:, 2].std():.3f}")
        
        print(f"=== åˆ†æå®Œæˆ ===\n")

    def convert_vggt_to_tram_camera(self, vggt_camera):
        """å¢å¼ºç‰ˆå›ºå®šåˆ†å¸ƒ - å¸¦è¯¦ç»†è°ƒè¯•ä¿¡æ¯"""
        batch_size = vggt_camera.shape[0]
        device = vggt_camera.device
        
        # print(f"ğŸ” === å¢å¼ºç‰ˆå›ºå®šåˆ†å¸ƒè½¬æ¢è°ƒè¯• ===")
        # print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # === 1. åˆ†æè¾“å…¥VGGTå‚æ•° ===
        vggt_translation = vggt_camera[:, :3]
        vggt_quaternion = vggt_camera[:, 3:7]
        vggt_fov = vggt_camera[:, 7:9]
        
        vggt_tx, vggt_ty, vggt_tz = vggt_translation.unbind(-1)
        
        # print(f"ğŸ“Š è¾“å…¥VGGTå‚æ•°ç»Ÿè®¡:")
        # print(f"  translation:")
        # print(f"    tx: [{vggt_tx.min():.6f}, {vggt_tx.max():.6f}], mean={vggt_tx.mean():.6f}Â±{vggt_tx.std():.6f}")
        # print(f"    ty: [{vggt_ty.min():.6f}, {vggt_ty.max():.6f}], mean={vggt_ty.mean():.6f}Â±{vggt_ty.std():.6f}")
        # print(f"    tz: [{vggt_tz.min():.3f}, {vggt_tz.max():.3f}], mean={vggt_tz.mean():.3f}Â±{vggt_tz.std():.3f}")
        # print(f"  quaternion: mean={vggt_quaternion.mean(dim=0)}")
        # print(f"  fov: [{torch.rad2deg(vggt_fov).min():.1f}Â°, {torch.rad2deg(vggt_fov).max():.1f}Â°], mean={torch.rad2deg(vggt_fov).mean():.1f}Â°")
        
        # === 2. ç”ŸæˆåŸºç¡€åˆ†å¸ƒ ===
        s_base = torch.normal(0.86, 0.17, (batch_size,), device=device)
        tx_base = torch.normal(0.19, 0.13, (batch_size,), device=device)
        ty_base = torch.normal(0.39, 0.33, (batch_size,), device=device)
        
        # print(f"ğŸ“Š åŸºç¡€åˆ†å¸ƒç”Ÿæˆ:")
        # print(f"  s_base: [{s_base.min():.3f}, {s_base.max():.3f}], mean={s_base.mean():.3f}Â±{s_base.std():.3f}")
        # print(f"  tx_base: [{tx_base.min():.3f}, {tx_base.max():.3f}], mean={tx_base.mean():.3f}Â±{tx_base.std():.3f}")
        # print(f"  ty_base: [{ty_base.min():.3f}, {ty_base.max():.3f}], mean={ty_base.mean():.3f}Â±{ty_base.std():.3f}")
        
        # === 3. è®¡ç®—VGGTå½±å“ ===
        depth_influence_weight = 0.05
        xy_influence_weight = 0.02
        
        vggt_tz_centered = vggt_tz - vggt_tz.mean()
        vggt_tx_centered = vggt_tx - vggt_tx.mean()
        vggt_ty_centered = vggt_ty - vggt_ty.mean()
        
        depth_influence = vggt_tz_centered * depth_influence_weight
        tx_influence = vggt_tx_centered * xy_influence_weight
        ty_influence = vggt_ty_centered * xy_influence_weight
        
        # print(f"ğŸ“Š VGGTå½±å“è®¡ç®—:")
        # print(f"  ä¸­å¿ƒåŒ–åçš„VGGTå˜åŒ–:")
        # print(f"    tz_centered: [{vggt_tz_centered.min():.3f}, {vggt_tz_centered.max():.3f}], std={vggt_tz_centered.std():.3f}")
        # print(f"    tx_centered: [{vggt_tx_centered.min():.6f}, {vggt_tx_centered.max():.6f}], std={vggt_tx_centered.std():.6f}")
        # print(f"    ty_centered: [{vggt_ty_centered.min():.6f}, {vggt_ty_centered.max():.6f}], std={vggt_ty_centered.std():.6f}")
        # print(f"  å½±å“é‡:")
        # print(f"    depth_influence: [{depth_influence.min():.4f}, {depth_influence.max():.4f}], mean={depth_influence.mean():.6f}")
        # print(f"    tx_influence: [{tx_influence.min():.6f}, {tx_influence.max():.6f}], mean={tx_influence.mean():.6f}")
        # print(f"    ty_influence: [{ty_influence.min():.6f}, {ty_influence.max():.6f}], mean={ty_influence.mean():.6f}")
        
        # === 4. åˆæˆæœ€ç»ˆå‚æ•° ===
        s_final = s_base + depth_influence
        tx_final = tx_base + tx_influence
        ty_final = ty_base + ty_influence
        
        # print(f"ğŸ“Š åˆæˆå‰å‚æ•°:")
        # print(f"  s_final: [{s_final.min():.3f}, {s_final.max():.3f}], mean={s_final.mean():.3f}Â±{s_final.std():.3f}")
        # print(f"  tx_final: [{tx_final.min():.3f}, {tx_final.max():.3f}], mean={tx_final.mean():.3f}Â±{tx_final.std():.3f}")
        # print(f"  ty_final: [{ty_final.min():.3f}, {ty_final.max():.3f}], mean={ty_final.mean():.3f}Â±{ty_final.std():.3f}")
        
        # === 5. Clampæ“ä½œ ===
        s_clamped = s_final.clamp(0.3, 1.5)
        tx_clamped = tx_final.clamp(-0.5, 0.8)
        ty_clamped = ty_final.clamp(0.0, 0.8)
        
        # ç»Ÿè®¡clampå½±å“
        s_clamp_count = (s_final != s_clamped).sum().item()
        tx_clamp_count = (tx_final != tx_clamped).sum().item()
        ty_clamp_count = (ty_final != ty_clamped).sum().item()
        
        # print(f"ğŸ“Š Clampæ“ä½œ:")
        # print(f"  s clamp: {s_clamp_count}/{batch_size} ä¸ªå€¼è¢«clamp")
        # print(f"  tx clamp: {tx_clamp_count}/{batch_size} ä¸ªå€¼è¢«clamp")
        # print(f"  ty clamp: {ty_clamp_count}/{batch_size} ä¸ªå€¼è¢«clamp")
        
        # === 6. æœ€ç»ˆç»“æœ ===
        s, tx, ty = s_clamped, tx_clamped, ty_clamped
        
        # print(f"ğŸ¯ æœ€ç»ˆè½¬æ¢ç»“æœ:")
        # print(f"  s: [{s.min():.3f}, {s.max():.3f}], mean={s.mean():.3f}Â±{s.std():.3f}")
        # print(f"  tx: [{tx.min():.3f}, {tx.max():.3f}], mean={tx.mean():.3f}Â±{tx.std():.3f}")
        # print(f"  ty: [{ty.min():.3f}, {ty.max():.3f}], mean={ty.mean():.3f}Â±{ty.std():.3f}")
        
        # === 7. ä¸ç›®æ ‡å¯¹æ¯” ===
        target_s, target_tx, target_ty = 0.86, 0.19, 0.39
        target_s_std, target_tx_std, target_ty_std = 0.17, 0.13, 0.33
        
        # print(f"ğŸ“ˆ ä¸ç›®æ ‡TRAMåˆ†å¸ƒå¯¹æ¯”:")
        # print(f"  ç›®æ ‡: s={target_s}Â±{target_s_std}, tx={target_tx}Â±{target_tx_std}, ty={target_ty}Â±{target_ty_std}")
        # print(f"  å®é™…: s={s.mean():.2f}Â±{s.std():.2f}, tx={tx.mean():.2f}Â±{tx.std():.2f}, ty={ty.mean():.2f}Â±{ty.std():.2f}")
        # print(f"  å‡å€¼è¯¯å·®: Î”s={abs(s.mean().item()-target_s):.3f}, Î”tx={abs(tx.mean().item()-target_tx):.3f}, Î”ty={abs(ty.mean().item()-target_ty):.3f}")
        # print(f"  æ ‡å‡†å·®æ¯”è¾ƒ: s_stdæ¯”ä¾‹={s.std().item()/target_s_std:.2f}, tx_stdæ¯”ä¾‹={tx.std().item()/target_tx_std:.2f}, ty_stdæ¯”ä¾‹={ty.std().item()/target_ty_std:.2f}")
        
        # === 8. VGGTä¿¡æ¯åˆ©ç”¨ç‡ ===
        vggt_contribution_s = depth_influence.abs().mean() / s.abs().mean() * 100
        vggt_contribution_tx = tx_influence.abs().mean() / tx.abs().mean() * 100 if tx.abs().mean() > 1e-6 else 0
        vggt_contribution_ty = ty_influence.abs().mean() / ty.abs().mean() * 100 if ty.abs().mean() > 1e-6 else 0
        
        # print(f"ğŸ“Š VGGTä¿¡æ¯åˆ©ç”¨ç‡:")
        # print(f"  så‚æ•°ä¸­VGGTæ·±åº¦çš„è´¡çŒ®: {vggt_contribution_s:.2f}%")
        # print(f"  txå‚æ•°ä¸­VGGTçš„è´¡çŒ®: {vggt_contribution_tx:.2f}%")
        # print(f"  tyå‚æ•°ä¸­VGGTçš„è´¡çŒ®: {vggt_contribution_ty:.2f}%")
        
        # print(f"=== å¢å¼ºç‰ˆè½¬æ¢è°ƒè¯•å®Œæˆ ===\n")
        
        return torch.stack([s, tx, ty], dim=1)

    def get_trainable_parameters(self):
        """è·å–å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
        trainable_params = 0
        total_params = 0
        
        components = {
            'TRAM Backbone': self.tram_backbone,
            'Feature Adapter': self.feature_adapter,  # æ·»åŠ è¿™è¡Œ
            'VGGT Camera Head': self.vggt_camera_head,
            'TRAM SMPL Head': self.tram_smpl_head
        }
        
        print(f"\n=== æ··åˆæ¶æ„å‚æ•°ç»Ÿè®¡ ===")
        for name, module in components.items():
            module_total = sum(p.numel() for p in module.parameters())
            module_trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)
            
            total_params += module_total
            trainable_params += module_trainable
            
            print(f"{name}: {module_total:,} ({module_total/1e6:.1f}M) æ€»å‚æ•°, "
                f"{module_trainable:,} ({module_trainable/1e6:.1f}M) å¯è®­ç»ƒ")
        
        print(f"\næ€»è®¡:")
        print(f"æ€»å‚æ•°: {total_params:,} ({total_params/1e6:.1f}M)")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/1e6:.1f}M)")
        print(f"å†»ç»“å‚æ•°: {total_params-trainable_params:,} ({(total_params-trainable_params)/1e6:.1f}M)")
        print(f"å¯è®­ç»ƒæ¯”ä¾‹: {trainable_params/total_params*100:.1f}%")
        
        return trainable_params, total_params
    
    def set_camera_fine_tune(self, enable: bool):
        """åŠ¨æ€è®¾ç½®ç›¸æœºå¤´å¾®è°ƒçŠ¶æ€"""
        for param in self.vggt_camera_head.parameters():
            param.requires_grad = enable
        self.camera_fine_tune = enable
        print(f"ç›¸æœºå¤´å¾®è°ƒçŠ¶æ€æ›´æ–°ä¸º: {enable}")
    
    def set_backbone_freeze(self, freeze: bool):
        """åŠ¨æ€è®¾ç½®éª¨å¹²ç½‘ç»œå†»ç»“çŠ¶æ€"""
        for param in self.vggt_aggregator.parameters():
            param.requires_grad = not freeze
        self.freeze_backbone = freeze
        print(f"éª¨å¹²ç½‘ç»œå†»ç»“çŠ¶æ€æ›´æ–°ä¸º: {freeze}")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("æµ‹è¯•åŸºäºçœŸå®ç»“æ„çš„VGGTè¿ç§»å­¦ä¹ æ¨¡å‹...")
    
    # æµ‹è¯•æƒé‡æå–
    tool = VGGTTransferLearning()
    backbone_path, camera_path, _ = tool.extract_and_save_weights()
    
    # æµ‹è¯•æ¨¡å‹
    model = TRAMWithVGGTTransfer(backbone_path, camera_path)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_input = torch.randn(2, 3, 224, 224)
    with torch.no_grad():
        output = model(test_input)
    
    print("è¾“å‡ºé”®:", list(output.keys()))
    for key, value in output.items():
        if isinstance(value, torch.Tensor):
            print(f"{key}: {value.shape}")
    
    # å‚æ•°ç»Ÿè®¡
    model.get_trainable_parameters()
    
    print("æ¨¡å—æµ‹è¯•å®Œæˆï¼")