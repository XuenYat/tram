import torch
import numpy as np
import sys
import os
from glob import glob
from pathlib import Path
import cv2
try:
    from scipy.spatial.transform import Rotation
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False
    print("Warning: scipy not available, 6DoF pose conversion will be limited")

# ç›´æ¥ä½¿ç”¨ç›¸å¯¹å¯¼å…¥path_utils
from ..utils.path_utils import ensure_vggt_in_path
ensure_vggt_in_path()

# VGGTç›¸å…³å¯¼å…¥
from vggt.models.vggt import VGGT
from vggt.utils.load_fn import load_and_preprocess_images
from vggt.utils.pose_enc import pose_encoding_to_extri_intri

# å°è¯•å¯¼å…¥å‡ ä½•å·¥å…·å‡½æ•°
try:
    from vggt.utils.geometry import unproject_depth_map_to_point_map
    HAS_VGGT_GEOMETRY = True
except ImportError:
    try:
        from vggt.utils.misc import unproject_depth_map_to_point_map
        HAS_VGGT_GEOMETRY = True
    except ImportError:
        HAS_VGGT_GEOMETRY = False
        print("Warning: VGGT geometry utils not found, will use simple unprojection")


class VGGTCameraEstimator:
    """VGGTç›¸æœºè½¨è¿¹ä¼°è®¡å°è£…ç±»"""
    
    def __init__(self, model_path="/root/.cache/torch/hub/checkpoints/model.pt"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
        self.model = None
        self.model_path = model_path
        
    def load_model(self):
        """åŠ è½½VGGTæ¨¡å‹ - ä½¿ç”¨æœ¬åœ°æ–‡ä»¶"""
        if self.model is None:
            print("ğŸš€ Loading VGGT model from local checkpoint...")
            self.model = VGGT()
            state_dict = torch.load(self.model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            self.model = self.model.to(self.device)
            self.model.eval()
            print(f"âœ… VGGT model loaded from {self.model_path} on {self.device}")
    
    def predict_poses_with_intrinsics(self, imgfiles, batch_size=4):
        """é¢„æµ‹ç›¸æœºposeså’Œå†…å‚çŸ©é˜µï¼ˆç”¨äºBundle Adjustmentï¼‰"""
        self.load_model()
        
        print(f"ğŸ“¸ Processing {len(imgfiles)} images with VGGT for poses+intrinsics (batch_size={batch_size})...")
        
        all_predictions = []
        
        # åˆ†æ‰¹å¤„ç†å›¾åƒ
        for i in range(0, len(imgfiles), batch_size):
            batch_files = imgfiles[i:i+batch_size]
            print(f"Processing batch {i//batch_size + 1}/{(len(imgfiles) + batch_size - 1)//batch_size}: frames {i}-{i+len(batch_files)-1}")
            
            # åŠ è½½å’Œé¢„å¤„ç†å½“å‰æ‰¹æ¬¡çš„å›¾åƒ
            images = load_and_preprocess_images(batch_files).to(self.device)
            
            # VGGTæ¨ç† - é¢„æµ‹ç›¸æœºå¤´å¹¶è·å–pose encoding
            with torch.no_grad():
                with torch.cuda.amp.autocast(dtype=self.dtype):
                    # æ·»åŠ batchç»´åº¦
                    images_batch = images[None] if images.dim() == 4 else images
                    
                    # è¿è¡Œaggregatorå’Œcamera_head
                    aggregated_tokens_list, ps_idx = self.model.aggregator(images_batch)
                    pose_enc = self.model.camera_head(aggregated_tokens_list)[-1]
                    
                    # åŒæ—¶è§£ç å¾—åˆ°å¤–å‚å’Œå†…å‚
                    H, W = images.shape[-2:]
                    extrinsics, intrinsics = pose_encoding_to_extri_intri(pose_enc, (H, W), build_intrinsics=True)
                    
                    # å°†ç»“æœåŒ…è£…æˆå­—å…¸æ ¼å¼
                    batch_predictions = {
                        'pose_enc': pose_enc,
                        'extrinsics': extrinsics,
                        'intrinsics': intrinsics
                    }
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            all_predictions.append(batch_predictions)
            
            # æ¸…ç†å½“å‰æ‰¹æ¬¡çš„æ˜¾å­˜
            del images
            torch.cuda.empty_cache()
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„é¢„æµ‹ç»“æœ
        combined_predictions = self._combine_batch_predictions(all_predictions)
        
        # æå–ç›¸æœºposeså’Œå†…å‚
        cam_R, cam_T = self._extract_camera_poses(combined_predictions, imgfiles)
        intrinsics = combined_predictions.get('intrinsics', None)
        
        if intrinsics is not None:
            # ç¡®ä¿å†…å‚çŸ©é˜µåœ¨CPUä¸Šå¹¶ä¸”å½¢çŠ¶æ­£ç¡®
            if len(intrinsics.shape) == 4:  # [batch, seq, 3, 3]
                intrinsics = intrinsics.squeeze(0)  # [seq, 3, 3]
            intrinsics = intrinsics.cpu()
            print(f"âœ… Extracted per-frame intrinsics: {intrinsics.shape}")
        else:
            print("âš ï¸ Could not extract intrinsics from VGGT output")
        
        return cam_R, cam_T, intrinsics
    
    def _extract_camera_poses(self, predictions, imgfiles):
        """ä»VGGTé¢„æµ‹ä¸­æå–ç›¸æœºposes"""
        print("ğŸ” Extracting camera poses from VGGT predictions...")
        
        # ç›´æ¥ä½¿ç”¨å·²è§£ç çš„extrinsicsï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨pose_enc
        if 'extrinsics' in predictions:
            print("âœ… Using pre-decoded extrinsics from predictions")
            extrinsics = predictions['extrinsics']
            
            # ç¡®ä¿ç§»é™¤batchç»´åº¦
            if len(extrinsics.shape) == 4 and extrinsics.shape[0] == 1:
                extrinsics = extrinsics.squeeze(0)  # [S, 3, 4]
            
            # æå–æ—‹è½¬çŸ©é˜µå’Œå¹³ç§»å‘é‡
            cam_R = extrinsics[:, :3, :3].float()  # (S, 3, 3)
            cam_T = extrinsics[:, :3, 3].float()   # (S, 3)
            
        elif 'pose_enc' in predictions:
            print("âš ï¸ Extrinsics not found, converting from pose_enc")
            pose_data = predictions['pose_enc']
            cam_R, cam_T = self._convert_pose_encoding(pose_data, imgfiles[0])
            
        else:
            raise ValueError("Neither 'extrinsics' nor 'pose_enc' found in predictions")
        
        print(f"âœ… Extracted {cam_R.shape[0]} camera poses")
        return cam_R, cam_T
    
    def _convert_pose_encoding(self, pose_data, first_img_path):
        """å°†VGGTçš„pose encodingè½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µå’Œå¹³ç§»å‘é‡"""
        
        # ç¡®ä¿ä¸ºfloat32ç±»å‹
        pose_data = pose_data.float()
        
        # è·å–å›¾åƒå°ºå¯¸
        first_img = cv2.imread(first_img_path)
        H, W = first_img.shape[:2]
        image_shape = (H, W)
        
        print(f"Converting pose encoding: {pose_data.shape} -> extrinsics")
        
        # å¤„ç†VGGTè¾“å‡ºçš„æ ‡å‡†æ ¼å¼ï¼š[B, S, 9] -> [S, 9]
        if len(pose_data.shape) == 3 and pose_data.shape[0] == 1:
            processed_pose_data = pose_data.squeeze(0)  # [S, 9]
        elif len(pose_data.shape) == 3:
            processed_pose_data = pose_data[0]  # å–ç¬¬ä¸€ä¸ªbatch
        elif len(pose_data.shape) == 2:
            processed_pose_data = pose_data  # å·²ç»æ˜¯ [S, 9] æ ¼å¼
        else:
            raise ValueError(f"Unexpected pose data shape: {pose_data.shape}")
        
        # éªŒè¯æ˜¯9D pose encoding
        if processed_pose_data.shape[-1] != 9:
            raise ValueError(f"Expected 9D pose encoding, got {processed_pose_data.shape[-1]}D")
        
        # ä½¿ç”¨å®˜æ–¹VGGTå‡½æ•°è½¬æ¢
        pose_encoding_with_batch = processed_pose_data.unsqueeze(0)  # [1, S, 9]
        extrinsic, _ = pose_encoding_to_extri_intri(pose_encoding_with_batch, image_shape)
        
        # æå–æ—‹è½¬å’Œå¹³ç§»
        extrinsic = extrinsic.squeeze(0)  # [S, 3, 4] 
        cam_R = extrinsic[:, :3, :3].float()  # (S, 3, 3)
        cam_T = extrinsic[:, :3, 3].float()   # (S, 3)
        
        print(f"âœ… Converted to cam_R: {cam_R.shape}, cam_T: {cam_T.shape}")
        return cam_R, cam_T
    
    def _combine_batch_predictions(self, all_predictions):
        """åˆå¹¶å¤šä¸ªæ‰¹æ¬¡çš„é¢„æµ‹ç»“æœ"""
        if len(all_predictions) == 1:
            return all_predictions[0]
        
        # åˆå¹¶å­—å…¸ç±»å‹çš„é¢„æµ‹ç»“æœ
        if isinstance(all_predictions[0], dict):
            combined = {}
            for key in all_predictions[0].keys():
                # æ”¶é›†æ‰€æœ‰æ‰¹æ¬¡ä¸­è¯¥keyå¯¹åº”çš„tensor
                tensors = []
                for pred in all_predictions:
                    if key in pred and isinstance(pred[key], torch.Tensor):
                        tensor = pred[key]
                        print(f"  Batch tensor {key}: {tensor.shape}")
                        
                        # ğŸ”§ ä¿®å¤ï¼šæ›´æ™ºèƒ½çš„batchç»´åº¦å¤„ç†
                        # VGGTè¾“å‡ºæ ¼å¼: [batch_size, num_frames, feature_dim]
                        # æˆ‘ä»¬éœ€è¦åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„å¸§ï¼Œæ‰€ä»¥è¦ç§»é™¤batchç»´åº¦
                        
                        if tensor.dim() == 3:
                            # 3ç»´tensorï¼Œæ£€æŸ¥ç¬¬ä¸€ç»´æ˜¯å¦ä¸ºbatchç»´åº¦
                            if tensor.shape[0] == 1:
                                # å¦‚æœbatch_size=1ï¼Œç§»é™¤batchç»´åº¦è¿›è¡Œå¸§çº§åˆ«æ‹¼æ¥
                                tensor = tensor.squeeze(0)
                                print(f"    Squeezed batch dim for {key}: {tensor.shape}")
                            else:
                                # å¦‚æœbatch_size>1ï¼Œè¯´æ˜è¿™ä¸€æ‰¹æœ‰å¤šå¸§ï¼Œä¿æŒåŸæ ·
                                # ä½†è¿™ç§æƒ…å†µåœ¨å½“å‰VGGTå®ç°ä¸­ä¸åº”è¯¥å‡ºç°
                                print(f"    Warning: {key} has batch_size > 1: {tensor.shape}")
                        elif tensor.dim() == 4:
                            # 4ç»´tensorï¼Œéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µå¤„ç†
                            if tensor.shape[0] == 1:
                                # ç§»é™¤batchç»´åº¦: [1, N, H, W] -> [N, H, W]
                                tensor = tensor.squeeze(0)
                                print(f"    Squeezed batch dim for {key}: {tensor.shape}")
                            else:
                                print(f"    Warning: {key} has batch_size > 1: {tensor.shape}")
                        # å¯¹äºå…¶ä»–ç»´åº¦çš„tensorï¼Œä¸åšä¿®æ”¹
                        
                        tensors.append(tensor)
                
                if tensors:
                    # æ²¿ç¬¬0ç»´ï¼ˆå¸§ç»´åº¦ï¼‰æ‹¼æ¥
                    try:
                        print(f"  Concatenating {key} tensors with shapes: {[t.shape for t in tensors]}")
                        combined[key] = torch.cat(tensors, dim=0)
                        print(f"  Result {key}: {combined[key].shape}")
                    except RuntimeError as e:
                        print(f"âŒ Failed to concatenate {key} tensors: {e}")
                        print(f"  Tensor shapes: {[t.shape for t in tensors]}")
                        # å¦‚æœæ‹¼æ¥å¤±è´¥ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªtensor
                        combined[key] = tensors[0]
                else:
                    # étensorç±»å‹ï¼Œå–ç¬¬ä¸€ä¸ª
                    combined[key] = all_predictions[0][key]
            return combined
        
        # åˆå¹¶tensorç±»å‹çš„é¢„æµ‹ç»“æœ
        elif isinstance(all_predictions[0], torch.Tensor):
            # å¤„ç†tensoråˆ—è¡¨
            print(f"Concatenating direct tensors with shapes: {[t.shape for t in all_predictions]}")
            try:
                result = torch.cat(all_predictions, dim=0)
                print(f"Result shape: {result.shape}")
                return result
            except RuntimeError as e:
                print(f"âŒ Failed to concatenate tensors: {e}")
                print(f"Tensor shapes: {[t.shape for t in all_predictions]}")
                # è¿”å›ç¬¬ä¸€ä¸ªtensor
                return all_predictions[0]
        
        else:
            raise ValueError(f"Unsupported prediction type: {type(all_predictions[0])}")
    
# å…¨å±€VGGTå®ä¾‹
_vggt_camera_estimator = None

def get_vggt_camera_estimator(model_path="/root/.cache/torch/hub/checkpoints/model.pt"):
    """è·å–å…¨å±€VGGTç›¸æœºä¼°è®¡å®ä¾‹"""
    global _vggt_camera_estimator
    if _vggt_camera_estimator is None:
        _vggt_camera_estimator = VGGTCameraEstimator(model_path)
    return _vggt_camera_estimator

def run_vggt_camera_estimation(img_folder, masks=None, calib=None, is_static=False, debug=False, debug_dir=None, batch_size=4):
    """
    VGGTç‰ˆæœ¬çš„ç›¸æœºè½¨è¿¹ä¼°è®¡ï¼Œæ›¿æ¢ä¼ ç»ŸSLAMæ–¹æ³•
    
    Args:
        img_folder: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
        masks: äººä½“mask (VGGTä¸éœ€è¦ï¼Œå¿½ç•¥)
        calib: ç›¸æœºå†…å‚ (VGGTå¯èƒ½ä¸éœ€è¦ï¼Œå¿½ç•¥)
        is_static: æ˜¯å¦é™æ€ç›¸æœº (VGGTè‡ªå·±åˆ¤æ–­ï¼Œå¿½ç•¥)
        debug: è°ƒè¯•æ¨¡å¼
        debug_dir: è°ƒè¯•è¾“å‡ºç›®å½•
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œç”¨äºæ§åˆ¶æ˜¾å­˜ä½¿ç”¨
    
    Returns:
        cam_R: torch.Tensor, shape (N, 3, 3) - æ—‹è½¬çŸ©é˜µ
        cam_T: torch.Tensor, shape (N, 3) - å¹³ç§»å‘é‡
        intrinsics: torch.Tensor, shape (N, 3, 3) - æ¯å¸§çš„å†…å‚çŸ©é˜µ
    """
    
    print("ğŸ¥ Running VGGT camera trajectory estimation...")
    
    # è·å–å›¾åƒæ–‡ä»¶åˆ—è¡¨
    imgfiles = sorted(glob(f'{img_folder}/*.jpg'))
    if len(imgfiles) == 0:
        raise ValueError(f"No jpg images found in {img_folder}")
    
    print(f"Found {len(imgfiles)} images")
    
    # ä½¿ç”¨VGGTé¢„æµ‹ç›¸æœºposeså’Œå†…å‚ (åˆ†æ‰¹å¤„ç†)
    vggt = get_vggt_camera_estimator()
    cam_R, cam_T, intrinsics = vggt.predict_poses_with_intrinsics(imgfiles, batch_size=batch_size)
    
    # è°ƒè¯•è¾“å‡º
    if debug and debug_dir is not None:
        _save_debug_info(cam_R, cam_T, imgfiles, debug_dir)
    
    return cam_R, cam_T, intrinsics

def _save_debug_info(cam_R, cam_T, imgfiles, debug_dir):
    """ä¿å­˜è°ƒè¯•ä¿¡æ¯"""
    debug_path = Path(debug_dir)
    debug_path.mkdir(exist_ok=True, parents=True)
    
    print(f"ğŸ’¾ Saving VGGT debug info to {debug_path}")
    
    # ä¿å­˜poses
    np.save(debug_path / 'vggt_cam_R.npy', cam_R.cpu().numpy())
    np.save(debug_path / 'vggt_cam_T.npy', cam_T.cpu().numpy())
    
    # åˆ†æè½¨è¿¹
    positions = cam_T.cpu().numpy()
    if len(positions) > 1:
        velocities = np.diff(positions, axis=0)
        speeds = np.linalg.norm(velocities, axis=1)
        
        debug_info = {
            'total_frames': len(imgfiles),
            'predicted_poses': len(cam_R),
            'mean_speed': float(np.mean(speeds)) if len(speeds) > 0 else 0.0,
            'std_speed': float(np.std(speeds)) if len(speeds) > 0 else 0.0,
            'max_speed': float(np.max(speeds)) if len(speeds) > 0 else 0.0,
            'trajectory_length': float(np.sum(speeds)) if len(speeds) > 0 else 0.0,
        }
        
        print(f"ğŸ“Š VGGT Trajectory stats:")
        print(f"   Frames: {debug_info['predicted_poses']}/{debug_info['total_frames']}")
        print(f"   Mean speed: {debug_info['mean_speed']:.4f}")
        print(f"   Speed variation: {debug_info['std_speed']:.4f}")
        print(f"   Total length: {debug_info['trajectory_length']:.4f}")
        
        np.save(debug_path / 'vggt_analysis.npy', debug_info)

def run_vggt_depth_estimation(img_folder, batch_size=4, model_path="/root/.cache/torch/hub/checkpoints/model.pt"):
    """
    VGGTæ·±åº¦å›¾ä¼°è®¡
    
    Args:
        img_folder: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
        batch_size: æ‰¹å¤„ç†å¤§å°
        model_path: VGGTæ¨¡å‹è·¯å¾„
        
    Returns:
        depth_maps: torch.Tensor, shape (N, H, W) - æ·±åº¦å›¾
        depth_conf: torch.Tensor, shape (N, H, W) - æ·±åº¦ç½®ä¿¡åº¦
    """
    
    print("ğŸ¯ Running VGGT depth estimation...")
    
    # è®¾å¤‡å’Œæ•°æ®ç±»å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16
    
    # åŠ è½½æ¨¡å‹
    model = VGGT()
    state_dict = torch.load(model_path, map_location=device)
    model.load_state_dict(state_dict)
    model = model.to(device)
    model.eval()
    
    # è·å–å›¾åƒæ–‡ä»¶
    imgfiles = sorted(glob(f'{img_folder}/*.jpg'))
    if len(imgfiles) == 0:
        raise ValueError(f"No images found in {img_folder}")
    
    print(f"Processing {len(imgfiles)} images for depth estimation...")
    
    all_depth_maps = []
    all_depth_conf = []
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(imgfiles), batch_size):
        batch_files = imgfiles[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(imgfiles) + batch_size - 1)//batch_size}: frames {i}-{i+len(batch_files)-1}")
        
        try:
            # åŠ è½½å›¾åƒ
            images = load_and_preprocess_images(batch_files).to(device)
            images = images[None]  # æ·»åŠ batchç»´åº¦
            
            with torch.no_grad():
                with torch.cuda.amp.autocast(dtype=dtype):
                    # è·å–èšåˆç‰¹å¾
                    aggregated_tokens_list, ps_idx = model.aggregator(images)
                    
                    # é¢„æµ‹æ·±åº¦å›¾
                    depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)
                    
                    # ç§»é™¤batchç»´åº¦å¹¶è½¬åˆ°CPU
                    if depth_map.shape[0] == 1:
                        depth_map = depth_map.squeeze(0)
                        depth_conf = depth_conf.squeeze(0)
                    
                    all_depth_maps.append(depth_map.cpu())
                    all_depth_conf.append(depth_conf.cpu())
            
        except Exception as e:
            print(f"Error processing batch {i//batch_size + 1}: {e}")
            continue
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
    if all_depth_maps:
        depth_maps = torch.cat(all_depth_maps, dim=0)
        depth_conf = torch.cat(all_depth_conf, dim=0)
        print(f"âœ… Depth estimation completed: {depth_maps.shape}")
        return depth_maps, depth_conf
    else:
        raise RuntimeError("No depth maps were successfully generated")

def run_vggt_point_estimation(img_folder, batch_size=4, model_path="/root/.cache/torch/hub/checkpoints/model.pt"):
    """
    VGGTç‚¹äº‘ä¼°è®¡
    
    Args:
        img_folder: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
        batch_size: æ‰¹å¤„ç†å¤§å°
        model_path: VGGTæ¨¡å‹è·¯å¾„
        
    Returns:
        point_maps: torch.Tensor, shape (N, H, W, 3) - ç‚¹äº‘å›¾
        point_conf: torch.Tensor, shape (N, H, W) - ç‚¹äº‘ç½®ä¿¡åº¦
    """
    
    print("ğŸ“ Running VGGT point cloud estimation...")
    
    # è®¾å¤‡å’Œæ•°æ®ç±»å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16
    
    # åŠ è½½æ¨¡å‹
    model = VGGT()
    state_dict = torch.load(model_path, map_location=device)
    model.load_state_dict(state_dict)
    model = model.to(device)
    model.eval()
    
    # è·å–å›¾åƒæ–‡ä»¶
    imgfiles = sorted(glob(f'{img_folder}/*.jpg'))
    if len(imgfiles) == 0:
        raise ValueError(f"No images found in {img_folder}")
    
    print(f"Processing {len(imgfiles)} images for point cloud estimation...")
    
    all_point_maps = []
    all_point_conf = []
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(imgfiles), batch_size):
        batch_files = imgfiles[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(imgfiles) + batch_size - 1)//batch_size}: frames {i}-{i+len(batch_files)-1}")
        
        try:
            # åŠ è½½å›¾åƒ
            images = load_and_preprocess_images(batch_files).to(device)
            images = images[None]  # æ·»åŠ batchç»´åº¦
            
            with torch.no_grad():
                with torch.cuda.amp.autocast(dtype=dtype):
                    # è·å–èšåˆç‰¹å¾
                    aggregated_tokens_list, ps_idx = model.aggregator(images)
                    
                    # é¢„æµ‹ç‚¹äº‘å›¾
                    point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)
                    
                    # ç§»é™¤batchç»´åº¦å¹¶è½¬åˆ°CPU
                    if point_map.shape[0] == 1:
                        point_map = point_map.squeeze(0)
                        point_conf = point_conf.squeeze(0)
                    
                    all_point_maps.append(point_map.cpu())
                    all_point_conf.append(point_conf.cpu())
            
        except Exception as e:
            print(f"Error processing batch {i//batch_size + 1}: {e}")
            continue
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
    if all_point_maps:
        point_maps = torch.cat(all_point_maps, dim=0)
        point_conf = torch.cat(all_point_conf, dim=0)
        print(f"âœ… Point cloud estimation completed: {point_maps.shape}")
        return point_maps, point_conf
    else:
        raise RuntimeError("No point maps were successfully generated")

def run_vggt_track_estimation(img_folder, query_points, batch_size=4, model_path="/root/.cache/torch/hub/checkpoints/model.pt"):
    """
    VGGTè½¨è¿¹è·Ÿè¸ª
    
    Args:
        img_folder: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
        query_points: torch.Tensor, shape (N, 2) - è¦è·Ÿè¸ªçš„æŸ¥è¯¢ç‚¹
        batch_size: æ‰¹å¤„ç†å¤§å°
        model_path: VGGTæ¨¡å‹è·¯å¾„
        
    Returns:
        track_list: è½¨è¿¹åˆ—è¡¨
        vis_score: å¯è§æ€§åˆ†æ•°
        conf_score: ç½®ä¿¡åº¦åˆ†æ•°
    """
    
    print("ğŸ¯ Running VGGT track estimation...")
    
    # è®¾å¤‡å’Œæ•°æ®ç±»å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16
    
    # åŠ è½½æ¨¡å‹
    model = VGGT()
    state_dict = torch.load(model_path, map_location=device)
    model.load_state_dict(state_dict)
    model = model.to(device)
    model.eval()
    
    # è·å–å›¾åƒæ–‡ä»¶
    imgfiles = sorted(glob(f'{img_folder}/*.jpg'))
    if len(imgfiles) == 0:
        raise ValueError(f"No images found in {img_folder}")
    
    print(f"Processing {len(imgfiles)} images for tracking {query_points.shape[0]} points...")
    
    # ç¡®ä¿query_pointsåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    query_points = query_points.to(device)
    
    all_track_lists = []
    all_vis_scores = []
    all_conf_scores = []
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(imgfiles), batch_size):
        batch_files = imgfiles[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(imgfiles) + batch_size - 1)//batch_size}: frames {i}-{i+len(batch_files)-1}")
        
        try:
            # åŠ è½½å›¾åƒ
            images = load_and_preprocess_images(batch_files).to(device)
            images = images[None]  # æ·»åŠ batchç»´åº¦
            
            with torch.no_grad():
                with torch.cuda.amp.autocast(dtype=dtype):
                    # è·å–èšåˆç‰¹å¾
                    aggregated_tokens_list, ps_idx = model.aggregator(images)
                    
                    # é¢„æµ‹è½¨è¿¹
                    track_list, vis_score, conf_score = model.track_head(
                        aggregated_tokens_list, images, ps_idx, query_points=query_points[None]
                    )
                    
                    # è½¬åˆ°CPUå¹¶æ”¶é›†ç»“æœ
                    all_track_lists.append(_move_tracks_to_cpu(track_list))
                    all_vis_scores.append(vis_score.cpu() if isinstance(vis_score, torch.Tensor) else vis_score)
                    all_conf_scores.append(conf_score.cpu() if isinstance(conf_score, torch.Tensor) else conf_score)
            
        except Exception as e:
            print(f"Error processing batch {i//batch_size + 1}: {e}")
            continue
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
    
    if all_track_lists:
        print(f"âœ… Track estimation completed for {len(all_track_lists)} batches")
        return all_track_lists, all_vis_scores, all_conf_scores
    else:
        raise RuntimeError("No tracks were successfully generated")

def run_vggt_depth_unprojection(img_folder, batch_size=4, model_path="/root/.cache/torch/hub/checkpoints/model.pt"):
    """
    VGGTæ·±åº¦å›¾åæŠ•å½±åˆ°3Dç‚¹äº‘ï¼ˆä½¿ç”¨æ·±åº¦+ç›¸æœºå‚æ•°ï¼‰
    è¿™é€šå¸¸æ¯”ç›´æ¥çš„ç‚¹äº‘é¢„æµ‹æ›´å‡†ç¡®
    
    Args:
        img_folder: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
        batch_size: æ‰¹å¤„ç†å¤§å°
        model_path: VGGTæ¨¡å‹è·¯å¾„
        
    Returns:
        point_maps_unprojected: torch.Tensor, shape (N, H, W, 3) - åæŠ•å½±çš„3Dç‚¹äº‘
    """
    
    print("ğŸ”„ Running VGGT depth unprojection to 3D points...")
    
    # è®¾å¤‡å’Œæ•°æ®ç±»å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16
    
    # åŠ è½½æ¨¡å‹
    model = VGGT()
    state_dict = torch.load(model_path, map_location=device)
    model.load_state_dict(state_dict)
    model = model.to(device)
    model.eval()
    
    # è·å–å›¾åƒæ–‡ä»¶
    imgfiles = sorted(glob(f'{img_folder}/*.jpg'))
    if len(imgfiles) == 0:
        raise ValueError(f"No images found in {img_folder}")
    
    print(f"Processing {len(imgfiles)} images for depth unprojection...")
    
    all_unprojected_points = []
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(imgfiles), batch_size):
        batch_files = imgfiles[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(imgfiles) + batch_size - 1)//batch_size}: frames {i}-{i+len(batch_files)-1}")
        
        try:
            # åŠ è½½å›¾åƒ
            images = load_and_preprocess_images(batch_files).to(device)
            images = images[None]  # æ·»åŠ batchç»´åº¦
            
            with torch.no_grad():
                with torch.cuda.amp.autocast(dtype=dtype):
                    # è·å–èšåˆç‰¹å¾
                    aggregated_tokens_list, ps_idx = model.aggregator(images)
                    
                    # é¢„æµ‹ç›¸æœºå‚æ•°
                    pose_enc = model.camera_head(aggregated_tokens_list)[-1]
                    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])
                    
                    # é¢„æµ‹æ·±åº¦å›¾
                    depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)
                    
                    # ä»æ·±åº¦å›¾åæŠ•å½±å¾—åˆ°3Dç‚¹äº‘
                    if not HAS_VGGT_GEOMETRY:
                        raise RuntimeError("VGGT geometry utils not available. Please install proper VGGT version.")
                    
                    point_map_unprojected = unproject_depth_map_to_point_map(
                        depth_map.squeeze(0), 
                        extrinsic.squeeze(0), 
                        intrinsic.squeeze(0)
                    )
                    
                    all_unprojected_points.append(point_map_unprojected.cpu())
            
        except Exception as e:
            print(f"Error processing batch {i//batch_size + 1}: {e}")
            continue
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
    if all_unprojected_points:
        point_maps_unprojected = torch.cat(all_unprojected_points, dim=0)
        print(f"âœ… Depth unprojection completed: {point_maps_unprojected.shape}")
        return point_maps_unprojected
    else:
        raise RuntimeError("No unprojected points were successfully generated")

def _move_tracks_to_cpu(tracks):
    """å°†trackså­—å…¸ä¸­çš„tensorç§»åŠ¨åˆ°CPU"""
    if isinstance(tracks, dict):
        result = {}
        for key, value in tracks.items():
            if isinstance(value, torch.Tensor):
                result[key] = value.cpu()
            elif isinstance(value, list):
                result[key] = [v.cpu() if isinstance(v, torch.Tensor) else v for v in value]
            else:
                result[key] = value
        return result
    elif isinstance(tracks, torch.Tensor):
        return tracks.cpu()
    else:
        return tracks

def run_vggt_unified_estimation(img_folder, 
                                batch_size=4, 
                                model_path="/root/.cache/torch/hub/checkpoints/model.pt",
                                outputs=['camera', 'depth', 'pointmap'],
                                query_points=None,
                                debug=False,
                                debug_dir=None):
    """
    VGGTç»Ÿä¸€ä¼°è®¡ï¼šä¸€æ¬¡æ€§è·å–æ‰€æœ‰éœ€è¦çš„æ•°æ®
    
    Args:
        img_folder: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
        batch_size: æ‰¹å¤„ç†å¤§å°
        model_path: VGGTæ¨¡å‹è·¯å¾„
        outputs: éœ€è¦çš„è¾“å‡ºåˆ—è¡¨ï¼Œå¯é€‰: ['camera', 'depth', 'pointmap', 'unprojection', 'tracking']
        query_points: torch.Tensor, shape (N, 2) - è·Ÿè¸ªç‚¹ï¼ˆä»…å½“'tracking'åœ¨outputsä¸­æ—¶éœ€è¦ï¼‰
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        debug_dir: è°ƒè¯•è¾“å‡ºç›®å½•
        
    Returns:
        results: dictï¼ŒåŒ…å«è¯·æ±‚çš„æ‰€æœ‰è¾“å‡º
            - 'camera': (cam_R, cam_T, intrinsics) - ç›¸æœºå§¿æ€å’Œå†…å‚
            - 'depth': (depth_maps, depth_conf) - æ·±åº¦å›¾å’Œç½®ä¿¡åº¦
            - 'pointmap': (point_maps, point_conf) - ç‚¹äº‘å›¾å’Œç½®ä¿¡åº¦  
            - 'unprojection': point_maps_unprojected - æ·±åº¦åæŠ•å½±ç‚¹äº‘
            - 'tracking': (track_lists, vis_scores, conf_scores) - è½¨è¿¹è·Ÿè¸ªç»“æœ
    """
    
    print(f"ğŸš€ Running VGGT unified estimation for outputs: {outputs}")
    
    # è®¾å¤‡å’Œæ•°æ®ç±»å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16
    
    # åŠ è½½æ¨¡å‹
    model = VGGT()
    state_dict = torch.load(model_path, map_location=device)
    model.load_state_dict(state_dict)
    model = model.to(device)
    model.eval()
    
    # è·å–å›¾åƒæ–‡ä»¶
    imgfiles = sorted(glob(f'{img_folder}/*.jpg'))
    if len(imgfiles) == 0:
        raise ValueError(f"No images found in {img_folder}")
    
    print(f"Processing {len(imgfiles)} images with unified estimation...")
    
    # æ£€æŸ¥è·Ÿè¸ªå‚æ•°
    if 'tracking' in outputs:
        if query_points is None:
            raise ValueError("query_points must be provided when 'tracking' is in outputs")
        query_points = query_points.to(device)
        print(f"Tracking {query_points.shape[0]} query points")
    
    # åˆå§‹åŒ–ç»“æœæ”¶é›†å™¨
    results = {}
    if 'camera' in outputs:
        all_pose_enc = []
        all_extrinsics = []
        all_intrinsics = []
    if 'depth' in outputs:
        all_depth_maps = []
        all_depth_conf = []
    if 'pointmap' in outputs:
        all_point_maps = []
        all_point_conf = []
    if 'unprojection' in outputs:
        all_unprojected_points = []
    if 'tracking' in outputs:
        all_track_lists = []
        all_vis_scores = []
        all_conf_scores = []
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(imgfiles), batch_size):
        batch_files = imgfiles[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(imgfiles) + batch_size - 1)//batch_size}: frames {i}-{i+len(batch_files)-1}")
        
        try:
            # åŠ è½½å›¾åƒ - åªåŠ è½½ä¸€æ¬¡ï¼
            images = load_and_preprocess_images(batch_files).to(device)
            
            with torch.no_grad():
                with torch.cuda.amp.autocast(dtype=dtype):
                    # æ·»åŠ batchç»´åº¦
                    images_batch = images[None]
                    
                    # å…±äº«çš„ç‰¹å¾èšåˆ - åªè®¡ç®—ä¸€æ¬¡ï¼
                    aggregated_tokens_list, ps_idx = model.aggregator(images_batch)
                    
                    # æ ¹æ®éœ€è¦é¢„æµ‹å„ç§è¾“å‡º
                    if 'camera' in outputs:
                        # é¢„æµ‹ç›¸æœºå‚æ•°
                        pose_enc = model.camera_head(aggregated_tokens_list)[-1]
                        # è§£ç å¤–å‚å’Œå†…å‚
                        H, W = images.shape[-2:]
                        extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, (H, W))
                        
                        all_pose_enc.append(pose_enc.cpu())
                        all_extrinsics.append(extrinsic.cpu())
                        all_intrinsics.append(intrinsic.cpu())
                    
                    if 'depth' in outputs:
                        # é¢„æµ‹æ·±åº¦å›¾
                        depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images_batch, ps_idx)
                        
                        # ç§»é™¤batchç»´åº¦
                        if depth_map.shape[0] == 1:
                            depth_map = depth_map.squeeze(0)
                            depth_conf = depth_conf.squeeze(0)
                        
                        all_depth_maps.append(depth_map.cpu())
                        all_depth_conf.append(depth_conf.cpu())
                    
                    if 'pointmap' in outputs:
                        # é¢„æµ‹ç‚¹äº‘å›¾
                        point_map, point_conf = model.point_head(aggregated_tokens_list, images_batch, ps_idx)
                        
                        # ç§»é™¤batchç»´åº¦
                        if point_map.shape[0] == 1:
                            point_map = point_map.squeeze(0)
                            point_conf = point_conf.squeeze(0)
                        
                        all_point_maps.append(point_map.cpu())
                        all_point_conf.append(point_conf.cpu())
                    
                    if 'unprojection' in outputs:
                        # éœ€è¦å…ˆè·å–ç›¸æœºå‚æ•°å’Œæ·±åº¦å›¾
                        if 'camera' not in outputs or 'depth' not in outputs:
                            # å¦‚æœæ²¡æœ‰åœ¨outputsä¸­ï¼Œä¸´æ—¶è®¡ç®—
                            if 'camera' not in outputs:
                                pose_enc_temp = model.camera_head(aggregated_tokens_list)[-1]
                                H, W = images.shape[-2:]
                                extrinsic_temp, intrinsic_temp = pose_encoding_to_extri_intri(pose_enc_temp, (H, W))
                            else:
                                extrinsic_temp, intrinsic_temp = extrinsic, intrinsic
                                
                            if 'depth' not in outputs:
                                depth_map_temp, _ = model.depth_head(aggregated_tokens_list, images_batch, ps_idx)
                            else:
                                depth_map_temp = depth_map
                        else:
                            extrinsic_temp, intrinsic_temp = extrinsic, intrinsic
                            depth_map_temp = depth_map
                        
                        # æ·±åº¦å›¾åæŠ•å½±åˆ°3Dç‚¹äº‘
                        if not HAS_VGGT_GEOMETRY:
                            print("Warning: VGGT geometry utils not available, skipping unprojection")
                        else:
                            point_map_unprojected = unproject_depth_map_to_point_map(
                                depth_map_temp.squeeze(0), 
                                extrinsic_temp.squeeze(0), 
                                intrinsic_temp.squeeze(0)
                            )
                            all_unprojected_points.append(point_map_unprojected.cpu())
                    
                    if 'tracking' in outputs:
                        # é¢„æµ‹è½¨è¿¹è·Ÿè¸ª
                        track_list, vis_score, conf_score = model.track_head(
                            aggregated_tokens_list, images_batch, ps_idx, query_points=query_points[None]
                        )
                        
                        # è½¬åˆ°CPUå¹¶æ”¶é›†ç»“æœ
                        all_track_lists.append(_move_tracks_to_cpu(track_list))
                        all_vis_scores.append(vis_score.cpu() if isinstance(vis_score, torch.Tensor) else vis_score)
                        all_conf_scores.append(conf_score.cpu() if isinstance(conf_score, torch.Tensor) else conf_score)
            
        except Exception as e:
            print(f"Error processing batch {i//batch_size + 1}: {e}")
            continue
        
        # æ¸…ç†æ˜¾å­˜
        del images
        torch.cuda.empty_cache()
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
    print("ğŸ“¦ Combining batch results...")
    
    if 'camera' in outputs and all_pose_enc:
        # åˆå¹¶ç›¸æœºæ•°æ®
        pose_enc_combined = torch.cat(all_pose_enc, dim=1) if len(all_pose_enc) > 1 else all_pose_enc[0]
        extrinsics_combined = torch.cat([e.squeeze(0) if e.shape[0] == 1 else e for e in all_extrinsics], dim=0)
        intrinsics_combined = torch.cat([i.squeeze(0) if i.shape[0] == 1 else i for i in all_intrinsics], dim=0)
        
        # æå–ç›¸æœºposes
        cam_R = extrinsics_combined[:, :3, :3].float()
        cam_T = extrinsics_combined[:, :3, 3].float()
        
        results['camera'] = (cam_R, cam_T, intrinsics_combined)
        print(f"âœ… Camera estimation completed: {cam_R.shape[0]} poses")
        
        # è°ƒè¯•è¾“å‡º
        if debug and debug_dir is not None:
            _save_debug_info(cam_R, cam_T, imgfiles, debug_dir)
    
    if 'depth' in outputs and all_depth_maps:
        depth_maps = torch.cat(all_depth_maps, dim=0)
        depth_conf = torch.cat(all_depth_conf, dim=0)
        results['depth'] = (depth_maps, depth_conf)
        print(f"âœ… Depth estimation completed: {depth_maps.shape}")
    
    if 'pointmap' in outputs and all_point_maps:
        point_maps = torch.cat(all_point_maps, dim=0)
        point_conf = torch.cat(all_point_conf, dim=0)
        results['pointmap'] = (point_maps, point_conf)
        print(f"âœ… Point cloud estimation completed: {point_maps.shape}")
    
    if 'unprojection' in outputs and all_unprojected_points:
        point_maps_unprojected = torch.cat(all_unprojected_points, dim=0)
        results['unprojection'] = point_maps_unprojected
        print(f"âœ… Depth unprojection completed: {point_maps_unprojected.shape}")
    
    if 'tracking' in outputs and all_track_lists:
        results['tracking'] = (all_track_lists, all_vis_scores, all_conf_scores)
        print(f"âœ… Track estimation completed for {len(all_track_lists)} batches")
    
    print(f"ğŸ‰ Unified VGGT estimation completed with outputs: {list(results.keys())}")
    return results